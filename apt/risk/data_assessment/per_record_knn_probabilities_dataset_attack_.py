"""
This module implements privacy risk assessment of synthetic datasets based on the paper:
"GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models" by D. Chen, N. Yu, Y. Zhang, M. Fritz
published in Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, 343â€“62, 2020.
https://doi.org/10.1145/3372297.3417238 and its implementation in https://github.com/DingfanChen/GAN-Leaks.
"""
from dataclasses import dataclass
from typing import Optional, Callable

import numpy as np
from sklearn.neighbors import NearestNeighbors

from apt.risk.data_assessment.attack_strategy_utils import KNNAttackStrategyUtils
from apt.risk.data_assessment.dataset_attack import DatasetAttackPerRecord, Config
from apt.risk.data_assessment.dataset_attack_result import DatasetAttackScore, DatasetAttackResultPerRecord
from apt.utils.datasets import ArrayDataset


@dataclass
class DatasetAttackConfigPerRecordKnnProbabilities(Config):
    """Configuration for DatasetAttackPerRecordKnnProbabilities.

    Attributes:
        k: Number of nearest neighbors to search
        use_batches: Divide query samples into batches or not.
        batch_size:  Query sample batch size.
        compute_distance: A callable function, which takes two arrays representing 1D vectors as inputs and must return
            one value indicating the distance between those vectors.
            See 'metric' parameter in sklearn.neighbors.NearestNeighbors documentation.
        distance_params:  Additional keyword arguments for the distance computation function, see 'metric_params' in
            sklearn.neighbors.NearestNeighbors documentation.
    """
    k: int = 5
    use_batches: bool = False
    batch_size: int = 10
    compute_distance: Callable = None
    distance_params: dict = None


@dataclass
class DatasetAttackScorePerRecordKnnProbabilities(DatasetAttackScore):
    """DatasetAttackPerRecordKnnProbabilities privacy score.
    Attributes
    ----------
    roc_auc_score :   the area under the receiver operating characteristic curve (AUC ROC) to evaluate the attack
                      performance.
    average_precision_score: the proportion of Predicted Positive cases that are correctly Real Positives (members)
    assessment_type : assessment type is 'PerRecordKnnProbabilities', to be used in reports
    """
    roc_auc_score: float
    average_precision_score: float
    assessment_type: str = 'PerRecordKnnProbabilities'


class DatasetAttackPerRecordKnnProbabilities(DatasetAttackPerRecord):
    """
         Privacy risk assessment for synthetic datasets based on Black-Box MIA attack using distances of
         members (training set) and non-members (holdout set) from their nearest neighbors in the synthetic dataset.
         By default, the Euclidean distance is used (L2 norm), but another compute_distance() method can be provided in
         configuration instead.
         The area under the receiver operating characteristic curve (AUC ROC) gives the privacy risk measure.
    """

    def __init__(self, original_data_members: ArrayDataset, original_data_non_members: ArrayDataset,
                 synthetic_data: ArrayDataset, dataset_name: str,
                 config: Optional[
                     DatasetAttackConfigPerRecordKnnProbabilities] = DatasetAttackConfigPerRecordKnnProbabilities()):
        """
        :param original_data_members: A container for the training original samples and labels
        :param original_data_non_members: A container for the holdout original samples and labels
        :param synthetic_data: A container for the synthetic samples and labels
        :param dataset_name: A name to identify this dataset
        :param config: Configuration parameters to guide the attack, optional
        """
        attack_strategy_utils = KNNAttackStrategyUtils(config.use_batches, config.batch_size)
        super().__init__(original_data_members, original_data_non_members, synthetic_data, dataset_name,
                         attack_strategy_utils, config)
        if config.compute_distance:
            self.nn_obj = NearestNeighbors(n_neighbors=config.k, algorithm='auto', metric=config.compute_distance,
                                           metric_params=config.distance_params)
        else:
            self.nn_obj = NearestNeighbors(n_neighbors=config.k, algorithm='auto')

    def assess_privacy(self) -> DatasetAttackResultPerRecord:
        """
        Membership Inference Attack which calculates probabilities of positive and negative samples to be generated by
        the synthetic data generator.
        The assumption is that since the generative model is trained to approximate the training data distribution
        then the probability of a sample to be a member of the training data should be proportional to the probability
        that the query sample can be generated by the generative model.
        The assumption is that if the probability that the query sample is generated by the generative model is large,
        it is more likely that the query sample was used to train the generative model. This probability is approximated
        by the Parzen window density estimation in 'probability_per_sample()', computed from the NN distances from the
        query samples to the synthetic data samples.

        :return
            :result of the attack with the probabilities of positive and negative samples to be generated by the
                synthetic data generator based on the NN distances from the query samples to the synthetic data samples
        """
        # nearest neighbor search
        self.attack_strategy_utils.fit(self.synthetic_data, self.nn_obj)

        # positive query
        pos_proba = self.attack_strategy_utils.find_knn(self.original_data_members, self.nn_obj,
                                                        self.probability_per_sample)

        # negative query
        neg_proba = self.attack_strategy_utils.find_knn(self.original_data_non_members, self.nn_obj,
                                                        self.probability_per_sample)

        result = DatasetAttackResultPerRecord(self.dataset_name, positive_probabilities=pos_proba,
                                              negative_probabilities=neg_proba)
        return result

    def calculate_privacy_score(self, dataset_attack_result: DatasetAttackResultPerRecord,
                                generate_plot=False) -> DatasetAttackScore:
        """
        Evaluate privacy score from the probabilities of positive and negative samples to be generated by the synthetic
        data generator. The probabilities are computed by the 'assess_privacy()' method.
        :param dataset_attack_result attack result containing probabilities of positive and negative samples to be
                generated by the synthetic data generator
        :param generate_plot generate AUC ROC curve plot and persist it
        :return
            :score of the attack, based on distance-based probabilities - mainly the ROC AUC score
        """
        pos_proba, neg_proba = \
            dataset_attack_result.positive_probabilities, dataset_attack_result.negative_probabilities
        fpr, tpr, threshold, auc, ap = self.calculate_metrics(pos_proba, neg_proba)
        score = DatasetAttackScorePerRecordKnnProbabilities(self.dataset_name, roc_auc_score=auc,
                                                            average_precision_score=ap)
        if generate_plot:
            self.plot_roc_curve(pos_proba, neg_proba)
        return score

    @staticmethod
    def probability_per_sample(distances: np.ndarray):
        """
        For every sample represented by its distance from the query sample to its KNN in synthetic data,
        computes the probability of the synthetic data to be part of the query dataset.
        :param distances: distance between every query sample in batch to its KNNs among synthetic samples
        :return
            distances: probability estimates of the query samples being generated and so being part of the synthetic set
        """
        return np.average(np.exp(-distances), axis=1)
